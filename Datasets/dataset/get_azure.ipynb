{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# nltk.download('punkt_tab')\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "class CustomAzureDataset:\n",
    "    def __init__(self, data_dir=\"\"):\n",
    "        self.data_dir = data_dir\n",
    "        self.splits = defaultdict(list)  # For train/test split data\n",
    "    \n",
    "    def load_image(self, image_path):\n",
    "        \"\"\"Load an image from a given path.\"\"\"\n",
    "        image = Image.open(image_path)\n",
    "        return image\n",
    "\n",
    "    def parse_annotation(self, annotation_path):\n",
    "        \"\"\"Parse the JSON annotation file.\"\"\"\n",
    "        with open(annotation_path, \"r\", encoding=\"utf8\") as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "    def load_dataset(self, filepath):\n",
    "        \"\"\"Load and store each sample in the dataset.\"\"\"\n",
    "        ann_dir = os.path.join(filepath, \"azure_results\")\n",
    "        img_dir = os.path.join(filepath, \"images\")\n",
    "\n",
    "        samples = []\n",
    "        for guid, file in enumerate(sorted(os.listdir(ann_dir))):\n",
    "            tokens = []\n",
    "            boxes = []\n",
    "            line_boxes = [] \n",
    "            handwritings = []\n",
    "            \n",
    "\n",
    "            # Load annotation\n",
    "            file_path = os.path.join(ann_dir, file)\n",
    "            data = self.parse_annotation(file_path)\n",
    "\n",
    "            # Load corresponding image\n",
    "            image_file = file.replace(\"json\", \"png\")\n",
    "            image_path = os.path.join(img_dir, image_file)\n",
    "            image = self.load_image(image_path)\n",
    "\n",
    "            # Extract tokens, boxes, and NER tags from annotation\n",
    "            for item in data[\"text_lines\"]:\n",
    "                words = item[\"words\"]\n",
    "                handwriting = item['handwriting']\n",
    "                line_bbox = item['line_bbox']\n",
    "\n",
    "                if len(words) == 0:\n",
    "                    continue\n",
    "                \n",
    "                for word in words:\n",
    "                    tokens.append(word['text'])\n",
    "                    boxes.append(word['bbox'])\n",
    "                    handwritings.append(handwriting)\n",
    "                    line_boxes.append(line_bbox)\n",
    "                    \n",
    "                \n",
    "                \n",
    "            \n",
    "\n",
    "            assert len(tokens) == len(boxes) == len(handwritings) == len(line_boxes) , \"Lengths of ner_tags, tokens, and boxes must be equal.\"\n",
    "            samples.append({\n",
    "                \"id\": str(guid),\n",
    "                \"tokens\": tokens,\n",
    "                'line_boxes': line_boxes,\n",
    "                \"bboxes\": boxes,\n",
    "                'handwritings': handwritings,\n",
    "                \"image\": image,\n",
    "                'image_name':file\n",
    "            })\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def split_generators(self):\n",
    "        \"\"\"Return train and test splits.\"\"\"\n",
    "        train_dir = os.path.join(self.data_dir, \"training_data\")\n",
    "        test_dir = os.path.join(self.data_dir, \"testing_data\")\n",
    "\n",
    "        # Load train and test data\n",
    "        self.splits[\"train\"] = self.load_dataset(train_dir)\n",
    "        self.splits[\"test\"] = self.load_dataset(test_dir)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Customize the printed representation of the dataset.\"\"\"\n",
    "        train_size = len(self.splits[\"train\"])\n",
    "        test_size = len(self.splits[\"test\"])\n",
    "\n",
    "        return (\n",
    "            f\"CustomFunsdDataset:\\n\"\n",
    "            f\"DatasetDict({{\\n\"\n",
    "            f\"    train: Dataset({{features: ['id', 'tokens', 'line_boxes', 'bboxes','handwritings' ,'image','image_name'], num_rows: {train_size}}}),\\n\"\n",
    "            f\"    test: Dataset({{features: ['id', 'tokens', 'line_boxes', 'bboxes', 'handwritings','image','image_name'], num_rows: {test_size}}})\\n\"\n",
    "            f\"}})\"\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, split):\n",
    "        \"\"\"Allow access to train or test splits like dataset['train'].\"\"\"\n",
    "        return self.splits[split]\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

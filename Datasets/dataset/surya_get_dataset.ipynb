{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\PX\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt_tab\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PX\\Master\\διπλωμαιτκ\\venv\\Lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PX\\Master\\διπλωμαιτκ\\venv\\Lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomFunsdDataset:\n",
       "DatasetDict({\n",
       "    train: Dataset({features: ['id', 'tokens', 'line_boxes', 'bboxes', 'ner_tags','line_ids','linkings','image','image_name'], num_rows: 149}),\n",
       "    test: Dataset({features: ['id', 'tokens', 'line_boxes', 'bboxes', 'ner_tags','line_ids','linkings','image','image_name'], num_rows: 50})\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "class CustomFunsdDataset:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.splits = defaultdict(list)  # For train/test split data\n",
    "    \n",
    "    def load_image(self, image_path):\n",
    "        \"\"\"Load an image from a given path.\"\"\"\n",
    "        image = Image.open(image_path)\n",
    "        return image\n",
    "\n",
    "    def parse_annotation(self, annotation_path):\n",
    "        \"\"\"Parse the JSON annotation file.\"\"\"\n",
    "        with open(annotation_path, \"r\", encoding=\"utf8\") as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "    def load_dataset(self, filepath):\n",
    "        \"\"\"Load and store each sample in the dataset.\"\"\"\n",
    "        ann_dir = os.path.join(filepath, \"surya_results\")\n",
    "        img_dir = os.path.join(filepath, \"images\")\n",
    "\n",
    "        samples = []\n",
    "        for guid, file in enumerate(sorted(os.listdir(ann_dir))):\n",
    "            tokens = []\n",
    "            boxes = []\n",
    "\n",
    "            # Load annotation\n",
    "            file_path = os.path.join(ann_dir, file)\n",
    "            data = self.parse_annotation(file_path)\n",
    "\n",
    "            # Load corresponding image\n",
    "            image_file = file.replace(\"json\", \"png\")\n",
    "            image_path = os.path.join(img_dir, image_file)\n",
    "            image = self.load_image(image_path)\n",
    "\n",
    "            for item in data['text_lines']: \n",
    "                wt = nlp.tokenizer(item['text'])    \n",
    "                tokens.extend(wt)\n",
    "                # Rejoin tokens that were split by slashes\n",
    "                split_them = ['/', ':']  # Punctuation to merge with adjacent tokens\n",
    "                not_split = ['&']        # Punctuation to remain unsplit\n",
    "                for i in range(len(wt)):\n",
    "                    boxes.append(item['bbox'])\n",
    "            assert len(tokens) == len(boxes)  , \"Lengths of ner_tags, tokens, and boxes must be equal.\"\n",
    "\n",
    "            samples.append({\n",
    "                \"id\": str(guid),\n",
    "                \"tokens\": tokens,\n",
    "                \"bboxes\": boxes,\n",
    "                \"image\": image,\n",
    "                'image_name':file\n",
    "            })\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def split_generators(self):\n",
    "        \"\"\"Return train and test splits.\"\"\"\n",
    "        train_dir = os.path.join(self.data_dir, \"training_data\")\n",
    "        test_dir = os.path.join(self.data_dir, \"testing_data\")\n",
    "\n",
    "        # Load train and test data\n",
    "        self.splits[\"train\"] = self.load_dataset(train_dir)\n",
    "        self.splits[\"test\"] = self.load_dataset(test_dir)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Customize the printed representation of the dataset.\"\"\"\n",
    "        train_size = len(self.splits[\"train\"])\n",
    "        test_size = len(self.splits[\"test\"])\n",
    "\n",
    "        return (\n",
    "            f\"CustomFunsdDataset:\\n\"\n",
    "            f\"DatasetDict({{\\n\"\n",
    "            f\"    train: Dataset({{features: ['id', 'tokens', 'line_boxes', 'bboxes', 'ner_tags','line_ids','linkings','image','image_name'], num_rows: {train_size}}}),\\n\"\n",
    "            f\"    test: Dataset({{features: ['id', 'tokens', 'line_boxes', 'bboxes', 'ner_tags','line_ids','linkings','image','image_name'], num_rows: {test_size}}})\\n\"\n",
    "            f\"}})\"\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, split):\n",
    "        \"\"\"Allow access to train or test splits like dataset['train'].\"\"\"\n",
    "        return self.splits[split]\n",
    "\n",
    "# Example usage\n",
    "data_dir = \"\"\n",
    "dataset = CustomFunsdDataset(data_dir)\n",
    "\n",
    "# Load the train and test splits\n",
    "dataset.split_generators()\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': [R&D,\n",
       "  QUALITY,\n",
       "  IMPROVEMENT,\n",
       "  SUGGESTION,\n",
       "  /,\n",
       "  SOLUTION,\n",
       "  FORM,\n",
       "  Name,\n",
       "  /,\n",
       "  Phone,\n",
       "  Ext,\n",
       "  .,\n",
       "  :,\n",
       "   ,\n",
       "  M.,\n",
       "  Hamann,\n",
       "  ,,\n",
       "  P.,\n",
       "  Harper,\n",
       "  ,,\n",
       "  P.,\n",
       "  Martinez,\n",
       "      ,\n",
       "  Date,\n",
       "  :,\n",
       "   ,\n",
       "  _,\n",
       "  9/3/92,\n",
       "  _,\n",
       "  R&D,\n",
       "  Group,\n",
       "  :,\n",
       "  _,\n",
       "  Licensee,\n",
       "  Supervisor,\n",
       "  /,\n",
       "  Manager,\n",
       "  :,\n",
       "  J.,\n",
       "  S.,\n",
       "  Wigand,\n",
       "  Discontinue,\n",
       "  coal,\n",
       "  retention,\n",
       "  analyses,\n",
       "  on,\n",
       "  licensee,\n",
       "  submitted,\n",
       "  Suggestion,\n",
       "  :,\n",
       "  product,\n",
       "  samples,\n",
       "  .,\n",
       "  (,\n",
       "  Note,\n",
       "  :,\n",
       "  Coal,\n",
       "  Retention,\n",
       "  testing,\n",
       "  is,\n",
       "  not,\n",
       "  performed,\n",
       "  by,\n",
       "  most,\n",
       "  licensees,\n",
       "  .,\n",
       "  Other,\n",
       "  B&W,\n",
       "  physical,\n",
       "  measurements,\n",
       "  as,\n",
       "  ends,\n",
       "  stability,\n",
       "  and,\n",
       "  inspection,\n",
       "  for,\n",
       "  soft,\n",
       "  spots,\n",
       "  in,\n",
       "  cigarettes,\n",
       "  are,\n",
       "  thought,\n",
       "  to,\n",
       "  be,\n",
       "  sufficient,\n",
       "  measures,\n",
       "  to,\n",
       "  assure,\n",
       "  cigarette,\n",
       "  physical,\n",
       "  integrity,\n",
       "  .,\n",
       "  The,\n",
       "  proposed,\n",
       "  action,\n",
       "  will,\n",
       "  Increase,\n",
       "  laboratory,\n",
       "  productivity,\n",
       "  .,\n",
       "  ),\n",
       "  Suggested,\n",
       "  Solution(s,\n",
       "  ):,\n",
       "   ,\n",
       "  Delete,\n",
       "  coal,\n",
       "  retention,\n",
       "  from,\n",
       "  the,\n",
       "  list,\n",
       "  of,\n",
       "  standard,\n",
       "  analyses,\n",
       "  performed,\n",
       "  on,\n",
       "  licensee,\n",
       "  submitted,\n",
       "  product,\n",
       "  samples,\n",
       "  .,\n",
       "  Special,\n",
       "  requests,\n",
       "  for,\n",
       "  coal,\n",
       "  retention,\n",
       "  testing,\n",
       "  could,\n",
       "  still,\n",
       "  be,\n",
       "  submitted,\n",
       "  on,\n",
       "  an,\n",
       "  exception,\n",
       "  basis,\n",
       "  .,\n",
       "  _,\n",
       "  _,\n",
       "  Yes,\n",
       "  _,\n",
       "  _,\n",
       "  No,\n",
       "  Have,\n",
       "  you,\n",
       "  contacted,\n",
       "  your,\n",
       "  Manager,\n",
       "  /,\n",
       "  Supervisor,\n",
       "  ?,\n",
       "  Manager,\n",
       "  Comments,\n",
       "  :,\n",
       "   ,\n",
       "  Manager,\n",
       "  ,,\n",
       "  please,\n",
       "  contact,\n",
       "  suggester,\n",
       "  and,\n",
       "  forward,\n",
       "  comments,\n",
       "  to,\n",
       "  the,\n",
       "  Quality,\n",
       "  Council,\n",
       "  .,\n",
       "  ---,\n",
       "  qip.wp,\n",
       "  5970057],\n",
       " 'bboxes': [[255.0, 202.0, 421.0, 214.0],\n",
       "  [255.0, 202.0, 421.0, 214.0],\n",
       "  [255.0, 202.0, 421.0, 214.0],\n",
       "  [252.0, 216.0, 426.0, 228.0],\n",
       "  [252.0, 216.0, 426.0, 228.0],\n",
       "  [252.0, 216.0, 426.0, 228.0],\n",
       "  [252.0, 216.0, 426.0, 228.0],\n",
       "  [88.0, 273.0, 603.0, 282.0],\n",
       "  [88.0, 273.0, 603.0, 282.0],\n",
       "  [88.0, 273.0, 603.0, 282.0],\n",
       "  [88.0, 273.0, 603.0, 282.0],\n",
       "  [88.0, 273.0, 603.0, 282.0],\n",
       "  [88.0, 273.0, 603.0, 282.0],\n",
       "  [88.0, 273.0, 603.0, 282.0],\n",
       "  [88.0, 273.0, 603.0, 282.0],\n",
       "  [88.0, 273.0, 603.0, 282.0],\n",
       "  [88.0, 273.0, 603.0, 282.0],\n",
       "  [88.0, 273.0, 603.0, 282.0],\n",
       "  [88.0, 273.0, 603.0, 282.0],\n",
       "  [88.0, 273.0, 603.0, 282.0],\n",
       "  [88.0, 273.0, 603.0, 282.0],\n",
       "  [88.0, 273.0, 603.0, 282.0],\n",
       "  [88.0, 273.0, 603.0, 282.0],\n",
       "  [88.0, 273.0, 603.0, 282.0],\n",
       "  [88.0, 273.0, 603.0, 282.0],\n",
       "  [88.0, 273.0, 603.0, 282.0],\n",
       "  [88.0, 273.0, 603.0, 282.0],\n",
       "  [88.0, 273.0, 603.0, 282.0],\n",
       "  [88.0, 273.0, 603.0, 282.0],\n",
       "  [417.0, 310.0, 577.0, 322.0],\n",
       "  [417.0, 310.0, 577.0, 322.0],\n",
       "  [417.0, 310.0, 577.0, 322.0],\n",
       "  [417.0, 310.0, 577.0, 322.0],\n",
       "  [417.0, 310.0, 577.0, 322.0],\n",
       "  [90.0, 315.0, 354.0, 325.0],\n",
       "  [90.0, 315.0, 354.0, 325.0],\n",
       "  [90.0, 315.0, 354.0, 325.0],\n",
       "  [90.0, 315.0, 354.0, 325.0],\n",
       "  [90.0, 315.0, 354.0, 325.0],\n",
       "  [90.0, 315.0, 354.0, 325.0],\n",
       "  [90.0, 315.0, 354.0, 325.0],\n",
       "  [187.0, 353.0, 594.0, 362.0],\n",
       "  [187.0, 353.0, 594.0, 362.0],\n",
       "  [187.0, 353.0, 594.0, 362.0],\n",
       "  [187.0, 353.0, 594.0, 362.0],\n",
       "  [187.0, 353.0, 594.0, 362.0],\n",
       "  [187.0, 353.0, 594.0, 362.0],\n",
       "  [187.0, 353.0, 594.0, 362.0],\n",
       "  [92.0, 356.0, 172.0, 370.0],\n",
       "  [92.0, 356.0, 172.0, 370.0],\n",
       "  [187.0, 367.0, 580.0, 376.0],\n",
       "  [187.0, 367.0, 580.0, 376.0],\n",
       "  [187.0, 367.0, 580.0, 376.0],\n",
       "  [187.0, 367.0, 580.0, 376.0],\n",
       "  [187.0, 367.0, 580.0, 376.0],\n",
       "  [187.0, 367.0, 580.0, 376.0],\n",
       "  [187.0, 367.0, 580.0, 376.0],\n",
       "  [187.0, 367.0, 580.0, 376.0],\n",
       "  [187.0, 367.0, 580.0, 376.0],\n",
       "  [187.0, 367.0, 580.0, 376.0],\n",
       "  [187.0, 367.0, 580.0, 376.0],\n",
       "  [187.0, 380.0, 532.0, 390.0],\n",
       "  [187.0, 380.0, 532.0, 390.0],\n",
       "  [187.0, 380.0, 532.0, 390.0],\n",
       "  [187.0, 380.0, 532.0, 390.0],\n",
       "  [187.0, 380.0, 532.0, 390.0],\n",
       "  [187.0, 380.0, 532.0, 390.0],\n",
       "  [187.0, 380.0, 532.0, 390.0],\n",
       "  [187.0, 380.0, 532.0, 390.0],\n",
       "  [187.0, 394.0, 574.0, 402.0],\n",
       "  [187.0, 394.0, 574.0, 402.0],\n",
       "  [187.0, 394.0, 574.0, 402.0],\n",
       "  [187.0, 394.0, 574.0, 402.0],\n",
       "  [187.0, 394.0, 574.0, 402.0],\n",
       "  [187.0, 394.0, 574.0, 402.0],\n",
       "  [187.0, 394.0, 574.0, 402.0],\n",
       "  [187.0, 394.0, 574.0, 402.0],\n",
       "  [189.0, 407.0, 594.0, 415.0],\n",
       "  [189.0, 407.0, 594.0, 415.0],\n",
       "  [189.0, 407.0, 594.0, 415.0],\n",
       "  [189.0, 407.0, 594.0, 415.0],\n",
       "  [189.0, 407.0, 594.0, 415.0],\n",
       "  [189.0, 407.0, 594.0, 415.0],\n",
       "  [189.0, 407.0, 594.0, 415.0],\n",
       "  [189.0, 407.0, 594.0, 415.0],\n",
       "  [189.0, 407.0, 594.0, 415.0],\n",
       "  [189.0, 420.0, 568.0, 430.0],\n",
       "  [189.0, 420.0, 568.0, 430.0],\n",
       "  [189.0, 420.0, 568.0, 430.0],\n",
       "  [189.0, 420.0, 568.0, 430.0],\n",
       "  [189.0, 420.0, 568.0, 430.0],\n",
       "  [189.0, 420.0, 568.0, 430.0],\n",
       "  [189.0, 420.0, 568.0, 430.0],\n",
       "  [189.0, 420.0, 568.0, 430.0],\n",
       "  [190.0, 433.0, 517.0, 444.0],\n",
       "  [190.0, 433.0, 517.0, 444.0],\n",
       "  [190.0, 433.0, 517.0, 444.0],\n",
       "  [190.0, 433.0, 517.0, 444.0],\n",
       "  [190.0, 433.0, 517.0, 444.0],\n",
       "  [190.0, 433.0, 517.0, 444.0],\n",
       "  [190.0, 433.0, 517.0, 444.0],\n",
       "  [93.0, 488.0, 596.0, 497.0],\n",
       "  [93.0, 488.0, 596.0, 497.0],\n",
       "  [93.0, 488.0, 596.0, 497.0],\n",
       "  [93.0, 488.0, 596.0, 497.0],\n",
       "  [93.0, 488.0, 596.0, 497.0],\n",
       "  [93.0, 488.0, 596.0, 497.0],\n",
       "  [93.0, 488.0, 596.0, 497.0],\n",
       "  [93.0, 488.0, 596.0, 497.0],\n",
       "  [93.0, 488.0, 596.0, 497.0],\n",
       "  [93.0, 488.0, 596.0, 497.0],\n",
       "  [93.0, 488.0, 596.0, 497.0],\n",
       "  [93.0, 488.0, 596.0, 497.0],\n",
       "  [260.0, 499.0, 549.0, 511.0],\n",
       "  [260.0, 499.0, 549.0, 511.0],\n",
       "  [260.0, 499.0, 549.0, 511.0],\n",
       "  [260.0, 499.0, 549.0, 511.0],\n",
       "  [260.0, 499.0, 549.0, 511.0],\n",
       "  [260.0, 513.0, 570.0, 524.0],\n",
       "  [260.0, 513.0, 570.0, 524.0],\n",
       "  [260.0, 513.0, 570.0, 524.0],\n",
       "  [260.0, 513.0, 570.0, 524.0],\n",
       "  [260.0, 513.0, 570.0, 524.0],\n",
       "  [260.0, 513.0, 570.0, 524.0],\n",
       "  [260.0, 513.0, 570.0, 524.0],\n",
       "  [260.0, 526.0, 584.0, 537.0],\n",
       "  [260.0, 526.0, 584.0, 537.0],\n",
       "  [260.0, 526.0, 584.0, 537.0],\n",
       "  [260.0, 526.0, 584.0, 537.0],\n",
       "  [260.0, 526.0, 584.0, 537.0],\n",
       "  [260.0, 526.0, 584.0, 537.0],\n",
       "  [260.0, 526.0, 584.0, 537.0],\n",
       "  [262.0, 539.0, 401.0, 554.0],\n",
       "  [262.0, 539.0, 401.0, 554.0],\n",
       "  [262.0, 539.0, 401.0, 554.0],\n",
       "  [262.0, 539.0, 401.0, 554.0],\n",
       "  [436.0, 605.0, 502.0, 620.0],\n",
       "  [436.0, 605.0, 502.0, 620.0],\n",
       "  [436.0, 605.0, 502.0, 620.0],\n",
       "  [524.0, 605.0, 563.0, 616.0],\n",
       "  [524.0, 605.0, 563.0, 616.0],\n",
       "  [524.0, 605.0, 563.0, 616.0],\n",
       "  [94.0, 607.0, 439.0, 621.0],\n",
       "  [94.0, 607.0, 439.0, 621.0],\n",
       "  [94.0, 607.0, 439.0, 621.0],\n",
       "  [94.0, 607.0, 439.0, 621.0],\n",
       "  [94.0, 607.0, 439.0, 621.0],\n",
       "  [94.0, 607.0, 439.0, 621.0],\n",
       "  [94.0, 607.0, 439.0, 621.0],\n",
       "  [94.0, 607.0, 439.0, 621.0],\n",
       "  [95.0, 650.0, 550.0, 659.0],\n",
       "  [95.0, 650.0, 550.0, 659.0],\n",
       "  [95.0, 650.0, 550.0, 659.0],\n",
       "  [95.0, 650.0, 550.0, 659.0],\n",
       "  [95.0, 650.0, 550.0, 659.0],\n",
       "  [95.0, 650.0, 550.0, 659.0],\n",
       "  [95.0, 650.0, 550.0, 659.0],\n",
       "  [95.0, 650.0, 550.0, 659.0],\n",
       "  [95.0, 650.0, 550.0, 659.0],\n",
       "  [95.0, 650.0, 550.0, 659.0],\n",
       "  [95.0, 650.0, 550.0, 659.0],\n",
       "  [97.0, 664.0, 326.0, 675.0],\n",
       "  [97.0, 664.0, 326.0, 675.0],\n",
       "  [97.0, 664.0, 326.0, 675.0],\n",
       "  [97.0, 664.0, 326.0, 675.0],\n",
       "  [97.0, 664.0, 326.0, 675.0],\n",
       "  [97.0, 664.0, 326.0, 675.0],\n",
       "  [260.0, 691.0, 440.0, 700.0],\n",
       "  [99.0, 823.0, 146.0, 837.0],\n",
       "  [525.0, 910.0, 608.0, 924.0]],\n",
       " 'image': <PIL.PngImagePlugin.PngImageFile image mode=L size=762x1000>,\n",
       " 'image_name': '0000971160.json'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
